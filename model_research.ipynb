{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have MIMIC-IV datasets downloaded, particularly discharge.csv and diagnoses.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000\n",
    "\n",
    "#Load MIMIC IV data (clinical notes and diagnoses)\n",
    "discharge_df = pd.read_csv('discharge.csv')\n",
    "diagnoses_df = pd.read_csv('diagnoses_icd.csv')  \n",
    "diagnoses_df = diagnoses_df.query(\"icd_version == 10\")\n",
    "\n",
    "#Filter clinical notes\n",
    "icd_10_hadm_ids = random.sample(list(set(diagnoses_df[\"hadm_id\"].values.tolist())), N)\n",
    "discharge_df = discharge_df[discharge_df[\"hadm_id\"].isin(icd_10_hadm_ids)]\n",
    "\n",
    "#Load filtered hadm and subject ids into new data frame\n",
    "hadm_to_subject_id = dict()\n",
    "for index, entry in diagnoses_df.iterrows():\n",
    "    if (entry[\"hadm_id\"] in hadm_to_subject_id):\n",
    "        continue\n",
    "    else:\n",
    "        hadm_to_subject_id[entry[\"hadm_id\"]] = entry[\"subject_id\"]\n",
    "icd_10_subject_ids = [hadm_to_subject_id[hadm_id] for hadm_id in icd_10_hadm_ids]\n",
    "query_df = pd.DataFrame()\n",
    "query_df[\"hadm_id\"] = icd_10_hadm_ids\n",
    "query_df[\"subject_id\"] = icd_10_subject_ids\n",
    "\n",
    "#Add ICD codes to new data frame\n",
    "hadm_to_icd = dict()\n",
    "for i in icd_10_hadm_ids:\n",
    "    icd_codes = diagnoses_df.loc[diagnoses_df['hadm_id'] == i, 'icd_code'].values.tolist()\n",
    "    icd_code_string = \"\"\n",
    "    for code in icd_codes:\n",
    "        icd_code_string += code + \" \"\n",
    "    hadm_to_icd[i] = icd_code_string.strip()\n",
    "icd_10_codes = [hadm_to_icd[hadm_id] for hadm_id in icd_10_hadm_ids]\n",
    "query_df[\"icd_codes\"] = icd_10_codes\n",
    "\n",
    "# text, ICD-10 mapping for eval purposes\n",
    "merged_df = pd.merge(discharge_df, query_df, on='hadm_id')\n",
    "eval_df = merged_df[['text', 'icd_codes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given OAI costs, we are using a subset, 500 randomly sampled examples\n",
    "\n",
    "# We also keep our notes used to around the average length, the average word count is:\n",
    "# Average word count: 1754.476, as seen a cell down below\n",
    "import random\n",
    "\n",
    "# Function to calculate word count\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Filter rows with word count under 2000\n",
    "filtered_df = eval_df[eval_df['text'].apply(count_words) < 2000]\n",
    "\n",
    "# Randomly sample 500 rows\n",
    "sampled_df = filtered_df.sample(n=500, random_state=42)\n",
    "\n",
    "# Display the sampled DataFrame\n",
    "sampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine longest texts for LLM context window purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by text length in descending order\n",
    "sorted_df = sampled_df.assign(text_length=sampled_df['text'].str.len()).sort_values('text_length', ascending=False)\n",
    "\n",
    "# Get the top 10 longest texts and their indices\n",
    "top_10_longest_texts = sorted_df.head(10)['text'].tolist()\n",
    "top_10_longest_texts_indices = sorted_df.head(10).index.tolist()\n",
    "\n",
    "# Print the top 10 longest texts with their indices\n",
    "# print(\"Top 10 longest text indices:\")\n",
    "# for i, (index, text) in enumerate(zip(top_10_longest_texts_indices, top_10_longest_texts)):\n",
    "#     print(f\"Index: {index}\")\n",
    "\n",
    "# Make a copy of the DataFrame to avoid SettingWithCopyWarning\n",
    "sampled_df_copy = sampled_df.copy()\n",
    "\n",
    "# Calculate word count for each text\n",
    "sampled_df_copy['word_count'] = sampled_df_copy['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Calculate average word count\n",
    "average_word_count = sampled_df_copy['word_count'].mean()\n",
    "print(f\"Average word count: {average_word_count}\")\n",
    "\n",
    "# Sort the DataFrame by word count in descending order\n",
    "sampled_df = sampled_df_copy.sort_values('word_count', ascending=False)\n",
    "\n",
    "# Get the top 10 texts with the highest word counts and their indices\n",
    "top_10_word_counts = sampled_df.head(10)['word_count'].tolist()\n",
    "top_10_word_counts_indices = sampled_df.head(10).index.tolist()\n",
    "\n",
    "# Print the top 10 texts with the highest word counts and their indices\n",
    "print(\"Top 10 highest word counts:\")\n",
    "for i, (index, word_count) in enumerate(zip(top_10_word_counts_indices, top_10_word_counts)):\n",
    "    print(f\"Index: {index}, Text {i+1}: {word_count} words\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine 50 most common ICD-10 codes in dataset, misc. dataset stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vvv supresses output for conciseness vvv\n",
    "%%capture\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Split the space-delimited ICD-10 codes into a list\n",
    "codes_list = sampled_df['icd_codes'].str.split()\n",
    "\n",
    "# Flatten the list of ICD-10 codes\n",
    "flattened_codes = [code for sublist in codes_list for code in sublist]\n",
    "\n",
    "# Calculate the number of medical codes per text\n",
    "num_codes_per_text = codes_list.apply(len)\n",
    "\n",
    "# Calculate the average number of medical codes per text\n",
    "average_codes_per_text = num_codes_per_text.mean()\n",
    "\n",
    "# Print the average number of medical codes per text\n",
    "print(f\"Average number of medical codes per text: {average_codes_per_text:.2f}\")\n",
    "\n",
    "# Count the occurrences of each ICD-10 code\n",
    "code_counter = Counter(flattened_codes)\n",
    "\n",
    "# Get the 50 most common ICD-10 codes\n",
    "top_50_common_codes = code_counter.most_common(50)\n",
    "\n",
    "# Create an ordered list of codes\n",
    "ordered_codes = [code for code, _ in top_50_common_codes]\n",
    "\n",
    "# Create a dictionary mapping code to frequency count\n",
    "code_freq_dict = {code: count for code, count in top_50_common_codes}\n",
    "\n",
    "# Print the ordered list of codes\n",
    "print(\"Ordered List of Codes:\")\n",
    "for i, code in enumerate(ordered_codes):\n",
    "    print(f\"{i+1}. {code}\")\n",
    "\n",
    "# Print the dictionary mapping code to frequency count\n",
    "print(\"\\nCode to Frequency Count Dictionary:\")\n",
    "for code, count in code_freq_dict.items():\n",
    "    print(f\"{code}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Predict top 16 codes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "\n",
    "top_16 = ordered_codes[:16]\n",
    "\n",
    "# Assuming that 'codes' column in dataframe and top_16 codes are space-delimited strings\n",
    "def process_codes(codes):\n",
    "    return set(codes.split())\n",
    "\n",
    "# Create binary representations for each code string and top_16\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([process_codes(code) for code in sampled_df['icd_codes']] + [set(top_16)])\n",
    "\n",
    "y_true = []\n",
    "for line in sampled_df['icd_codes']:\n",
    "    y_true.append(line.split())\n",
    "    \n",
    "y_true = mlb.transform(y_true)\n",
    "y_pred = mlb.transform([set(top_16) for _ in range(500)])\n",
    "\n",
    "# Calculate metrics\n",
    "micro_auc = roc_auc_score(y_true, y_pred, average='micro')\n",
    "macro_auc = roc_auc_score(y_true, y_pred, average='macro')\n",
    "micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "\n",
    "def precision_at_k(y_true, y_pred, k):\n",
    "    precisions = []\n",
    "    for true_codes in y_true:\n",
    "        top_k_preds = y_pred[:k]  # Get the top k predictions\n",
    "        # Count the number of correct predictions\n",
    "        correct_preds = sum([1 for code in top_k_preds if code in true_codes])\n",
    "        # Calculate precision and append it to the list\n",
    "        precisions.append(correct_preds / len(top_k_preds))\n",
    "    return np.mean(precisions)\n",
    "\n",
    "\n",
    "# Convert y_true back to the set representation\n",
    "y_true_sets = [set(codes) for codes in mlb.inverse_transform(y_true)]\n",
    "\n",
    "# The top_16 codes\n",
    "y_pred = top_16\n",
    "\n",
    "precision_at_5 = precision_at_k(y_true_sets, y_pred, 5)\n",
    "\n",
    "# specified\n",
    "print('Micro AUC:', micro_auc)\n",
    "print('Macro AUC:', macro_auc)\n",
    "print('Micro F1:', micro_f1)\n",
    "print('Macro F1:', macro_f1)\n",
    "print('Precision P@5:', precision_at_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: GPT 3.5 eval (turbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard GPT-3.5/4\n",
    "\n",
    "# for Azure\n",
    "# openai.api_type = \"azure\"\n",
    "# openai.api_key = \"...\"\n",
    "# openai.api_base = \"https://example-endpoint.openai.azure.com\"\n",
    "# openai.api_version = \"2023-03-15-preview\"\n",
    "\n",
    "def call_model(prompt):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\", \n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=200,           # Maximum number of tokens in the generated text. If the generated response exceeds this limit, it will be cut off.\n",
    "        temperature=1,          # Controls randomness. Higher values (closer to 1) make output more random, lower values (closer to 0) make it more deterministic.\n",
    "        top_p=1,                  # Sets the nucleus sampling value, controls diversity via probability threshold, can be used as an alternative to temperature.\n",
    "        frequency_penalty=0.0,    # Penalizes new tokens based on their frequency in the model's training data. Ranges from -2.0 to 2.0.\n",
    "        presence_penalty=0.0,     # Penalizes new tokens based on whether they appear in the context. Ranges from -2.0 to 2.0.\n",
    "        n=1,                      # The number of completions to generate. More completions means more diversity, but at a higher computational cost.\n",
    "        stream=False,             # If true, generate the response as a stream to reduce latency.\n",
    "        stop=None,                # A sequence (or list of sequences) where the API will stop generating further tokens.\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "import openai\n",
    "openai.api_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/Users/roshanswaroop/rema/rema/codemCodes.json', 'r') as f:\n",
    "    # Load JSON data from file\n",
    "    legit_codes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt ='You are an experienced medical coder. You must identify all correct ICD-10 codes for the following clinical note. Pay attention to areas describing present illness, chart review, imaging, discharge labs, active issues, medications, chief complaint, major surgery/procedure, etc. Return your answer in the following format, but note that the actual correct codes may vary greatly from these: I10, E78.5, Z87.891\\n'\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_descriptions(index):\n",
    "    # Get the row corresponding to the index\n",
    "    row = eval_df.loc[index]\n",
    "\n",
    "    # Extract the text and ICD codes from the row\n",
    "    text = row['text']\n",
    "    icd_codes = row['icd_codes'].split()\n",
    "\n",
    "\n",
    "    code_list = []\n",
    "    description_list = []\n",
    "    # For each ICD code, print the code and its description\n",
    "    for code in icd_codes:\n",
    "        description = icd_code_descriptions.get(code, \"No description available\")\n",
    "        code_list.append(code)\n",
    "        description_list.append(description)\n",
    "\n",
    "    return code_list, description_list\n",
    "\n",
    "def print_code_descriptions(index):\n",
    "    # Get the row corresponding to the index\n",
    "    row = eval_df.loc[index]\n",
    "\n",
    "    # Extract the text and ICD codes from the row\n",
    "    text = row['text']\n",
    "    icd_codes = row['icd_codes'].split()\n",
    "\n",
    "    #print(\"Text:\", text, \"\\n\")\n",
    "\n",
    "    \n",
    "    code_list = []\n",
    "    description_list = []\n",
    "    # For each ICD code, print the code and its description\n",
    "    for code in icd_codes:\n",
    "        description = icd_code_descriptions.get(code, \"No description available\")\n",
    "        code_list.append(code)\n",
    "        description_list.append(description)\n",
    "        \n",
    "    print(\"Code:\", code_list)\n",
    "    print(\"Description:\", description_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Empty dictionary to store the mapping\n",
    "results = {'Original Codes': [], 'Predicted Codes': []}\n",
    "\n",
    "for i in range(5):\n",
    "    note = sampled_df[['text']].iloc[i][0]\n",
    "    inference = call_model(prompt + note)\n",
    "\n",
    "    # Append original codes and descriptions to the results\n",
    "    original_codes, _ = get_code_descriptions(i)\n",
    "    results['Original Codes'].append(original_codes)\n",
    "\n",
    "    # Predicted codes are inferred from the model and need to be processed to match the format of original codes\n",
    "    predicted_codes = inference.replace(\",\", \" \").split()\n",
    "\n",
    "    # Check each predicted code against the list of legit codes before appending\n",
    "    # Removing periods from predicted codes only for lookup\n",
    "    legit_predicted_codes = [code for code in predicted_codes if code.replace(\".\", \"\") in legit_codes]\n",
    "    results['Predicted Codes'].append(legit_predicted_codes)\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the simplified codes\n",
    "original_codes_simplified = results_df['Original Codes'].apply(lambda codes: [code[:3] for code in codes])\n",
    "predicted_codes_simplified = results_df['Predicted Codes'].apply(lambda codes: [code[:3] for code in codes])\n",
    "\n",
    "# Create binary representations for each code\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Get all unique codes in the dataset\n",
    "all_codes = set()\n",
    "for codes in original_codes_simplified:\n",
    "    all_codes.update(codes)\n",
    "for codes in predicted_codes_simplified:\n",
    "    all_codes.update(codes)\n",
    "\n",
    "#print(all_codes)\n",
    "# Fit the binarizer on all unique codes\n",
    "mlb.fit([all_codes])\n",
    "\n",
    "# Now transform your labels\n",
    "print(original_codes_simplified, predicted_codes_simplified)\n",
    "y_true = []\n",
    "for line in original_codes_simplified:\n",
    "    y_true.append(line)\n",
    "#print(y_true)\n",
    "y_true = mlb.transform(y_true)\n",
    "y_pred = mlb.transform(predicted_codes_simplified)\n",
    "\n",
    "#print(y_true)\n",
    "\n",
    "\n",
    "# Calculate metrics\n",
    "micro_auc = roc_auc_score(y_true, y_pred, average='micro')\n",
    "auc_scores = []\n",
    "for class_index in range(y_true.shape[1]):\n",
    "    try:\n",
    "        class_auc = roc_auc_score(y_true[:, class_index], y_pred[:, class_index])\n",
    "        auc_scores.append(class_auc)\n",
    "    except ValueError:\n",
    "        auc_scores.append(0.5)  # means it's as good as random for that class instance\n",
    "macro_auc = np.mean(auc_scores)\n",
    "micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "# Convert y_true back to the set representation\n",
    "y_true_sets = [set(codes) for codes in mlb.inverse_transform(y_true)]\n",
    "y_pred_sets = [set(codes) for codes in mlb.inverse_transform(y_pred)]\n",
    "\n",
    "# Calculate precision at 5\n",
    "precision_at_5 = precision_at_k(y_true_sets, y_pred_sets, 5)\n",
    "\n",
    "# Print the results\n",
    "print('Micro AUC:', micro_auc)\n",
    "print('Macro AUC:', macro_auc)\n",
    "print('Micro F1:', micro_f1)\n",
    "print('Macro F1:', macro_f1)\n",
    "print('Precision P@5:', precision_at_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_sums = np.sum(y_true, axis=0)\n",
    "if np.any(column_sums == len(y_true)) or np.any(column_sums == 0):\n",
    "    print('There is a class with only one type of instance.')\n",
    "else:\n",
    "    print('Every class has at least one positive and one negative instance.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame(original_codes_simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(predicted_codes_simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input an index, get MIMIC IV's code suggestions and the original clinical text\n",
    "print_code_descriptions(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4 zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = ''\n",
    "\n",
    "def call_model(prompt):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\", \n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=200,           # Maximum number of tokens in the generated text. If the generated response exceeds this limit, it will be cut off.\n",
    "        temperature=1,          # Controls randomness. Higher values (closer to 1) make output more random, lower values (closer to 0) make it more deterministic.\n",
    "        top_p=1,                  # Sets the nucleus sampling value, controls diversity via probability threshold, can be used as an alternative to temperature.\n",
    "        frequency_penalty=0.0,    # Penalizes new tokens based on their frequency in the model's training data. Ranges from -2.0 to 2.0.\n",
    "        presence_penalty=0.0,     # Penalizes new tokens based on whether they appear in the context. Ranges from -2.0 to 2.0.\n",
    "        n=1,                      # The number of completions to generate. More completions means more diversity, but at a higher computational cost.\n",
    "        stream=False,             # If true, generate the response as a stream to reduce latency.\n",
    "        stop=None,                # A sequence (or list of sequences) where the API will stop generating further tokens.\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "prompt ='You are an experienced medical coder. You must identify all correct ICD-10 codes for the following clinical note. Pay attention to areas describing present illness, chart review, imaging, discharge labs, active issues, medications, chief complaint, major surgery/procedure, etc. Return your answer in the following format, but note that the actual correct codes may vary greatly from these: I10, E78.5, Z87.891\\n'\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "def get_code_descriptions(index):\n",
    "    # Get the row corresponding to the index\n",
    "    row = eval_df.loc[index]\n",
    "\n",
    "    # Extract the text and ICD codes from the row\n",
    "    text = row['text']\n",
    "    icd_codes = row['icd_codes'].split()\n",
    "\n",
    "\n",
    "    code_list = []\n",
    "    description_list = []\n",
    "    # For each ICD code, print the code and its description\n",
    "    for code in icd_codes:\n",
    "        description = icd_code_descriptions.get(code, \"No description available\")\n",
    "        code_list.append(code)\n",
    "        description_list.append(description)\n",
    "\n",
    "    return code_list, description_list\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Empty dictionary to store the mapping\n",
    "results = {'Original Codes': [], 'Predicted Codes': []}\n",
    "\n",
    "for i in range(5):\n",
    "    note = sampled_df[['text']].iloc[i][0]\n",
    "    inference = call_model(prompt + note)\n",
    "\n",
    "    # Append original codes and descriptions to the results\n",
    "    original_codes, _ = get_code_descriptions(i)\n",
    "    results['Original Codes'].append(original_codes)\n",
    "\n",
    "    # Predicted codes are inferred from the model and need to be processed to match the format of original codes\n",
    "    predicted_codes = inference.replace(\",\", \" \").split()\n",
    "\n",
    "    # Check each predicted code against the list of legit codes before appending\n",
    "    # Removing periods from predicted codes only for lookup\n",
    "    legit_predicted_codes = [code for code in predicted_codes if code.replace(\".\", \"\") in legit_codes]\n",
    "    results['Predicted Codes'].append(legit_predicted_codes)\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def evaluate(results_df):  \n",
    "    # Get the simplified codes\n",
    "    original_codes_simplified = results_df['Original Codes'].apply(lambda codes: [code[:3] for code in codes])\n",
    "    predicted_codes_simplified = results_df['Predicted Codes'].apply(lambda codes: [code[:3] for code in codes])\n",
    "\n",
    "    # Create binary representations for each code\n",
    "    mlb = MultiLabelBinarizer()\n",
    "\n",
    "    # Get all unique codes in the dataset\n",
    "    all_codes = set()\n",
    "    for codes in original_codes_simplified:\n",
    "        all_codes.update(codes)\n",
    "    for codes in predicted_codes_simplified:\n",
    "        all_codes.update(codes)\n",
    "\n",
    "    #print(all_codes)\n",
    "    # Fit the binarizer on all unique codes\n",
    "    mlb.fit([all_codes])\n",
    "\n",
    "    # Now transform your labels\n",
    "    #print(original_codes_simplified, predicted_codes_simplified)\n",
    "    y_true = []\n",
    "    for line in original_codes_simplified:\n",
    "        y_true.append(line)\n",
    "    #print(y_true)\n",
    "    y_true = mlb.transform(y_true)\n",
    "    y_pred = mlb.transform(predicted_codes_simplified)\n",
    "\n",
    "    #print(y_true)\n",
    "\n",
    "\n",
    "    # Calculate metrics\n",
    "    micro_auc = roc_auc_score(y_true, y_pred, average='micro')\n",
    "    auc_scores = []\n",
    "    for class_index in range(y_true.shape[1]):\n",
    "        try:\n",
    "            class_auc = roc_auc_score(y_true[:, class_index], y_pred[:, class_index])\n",
    "            auc_scores.append(class_auc)\n",
    "        except ValueError:\n",
    "            auc_scores.append(0.5)  # means it's as good as random for that class instance\n",
    "    macro_auc = np.mean(auc_scores)\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    # Convert y_true back to the set representation\n",
    "    y_true_sets = [set(codes) for codes in mlb.inverse_transform(y_true)]\n",
    "    y_pred_sets = [set(codes) for codes in mlb.inverse_transform(y_pred)]\n",
    "\n",
    "    # Calculate precision at 5\n",
    "    precision_at_5 = precision_at_k(y_true_sets, y_pred_sets, 5)\n",
    "\n",
    "    # Print the results\n",
    "    print('Micro AUC:', micro_auc)\n",
    "    print('Macro AUC:', macro_auc)\n",
    "    print('Micro F1:', micro_f1)\n",
    "    print('Macro F1:', macro_f1)\n",
    "    print('Precision P@5:', precision_at_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_code_descriptions(results_df, index):\n",
    "    # Get the row corresponding to the index\n",
    "    row = results_df.loc[index]\n",
    "\n",
    "    # Extract the original and predicted ICD codes from the row\n",
    "    original_codes = row['Original Codes']\n",
    "    predicted_codes = row['Predicted Codes']\n",
    "\n",
    "    # For each ICD code in original and predicted codes, print the code and its description\n",
    "    for code_list, label in zip([original_codes, predicted_codes], ['Original', 'Predicted']):\n",
    "        print(f\"{label} Codes and Descriptions:\")\n",
    "        for code in code_list:\n",
    "            description = icd_code_descriptions.get(code.replace(\".\", \"\"), \"No description available\")\n",
    "            print(\"Code:\", code)\n",
    "            print(\"Description:\", description)\n",
    "        print(\"\\n\")\n",
    "\n",
    "print_code_descriptions(results_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT 4 arbitrary few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot = 'You are an experienced medical coder. You must identify all correct ICD-10 codes for the following clinical note. Pay attention to areas describing present illness, chart review, imaging, discharge labs, active issues, medications, chief complaint, major surgery/procedure. Return your answer in the following format, but note that the actual correct codes may vary greatly from these: I10, E78.5, Z87.891. Here are a few examples: \\n Patient admits to a history of alcohol dependence. Consuming 5 – 6 beers per day now, down from 10 – 12 per day 6 months ago. States that he has nausea and sweating with “the shakes” when he does not drink. \\n R10.819 \\n Patient stopped taking olmesartan medoxomil due to side effects, including a headache that began after starting the medication and still exists, and tiredness. \\n T46.5X6A, Z91.128 \\n  38 year old established female seen one week ago for decreased exercise tolerance and general malaise over the past four weeks when doing her daily aerobics class. Labs were ordered on that visit. She presents today with pale skin, weakness, and epigastric pain; symptoms are unchanged since previous visit. Laboratory studies reviewed today are as follows: HGB 8.5 gm/dL, HCT 27%, platelets 300,000/mm3, reticulocytes 0.24%, MCV 75, serum iron 41 mcg/dL, serum ferritin 9 ng/ml, TIBC 457 mcg/dL; Fecal occult blood test is positive. She takes Esomeprazole daily for GERD with esophagitis and reports taking OTC antacids at bedtime for epigastric pain for the past three months. She also uses ibuprofen as needed for headaches. \\n D50.0, K21.0'\n",
    "\n",
    "print(few_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = {'Original Codes': [], 'Predicted Codes': []}\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    note = sampled_df[['text']].iloc[i][0]\n",
    "    inference = call_model(few_shot + note)\n",
    "\n",
    "    # Append original codes and descriptions to the results\n",
    "    original_codes, _ = get_code_descriptions(i)\n",
    "    results2['Original Codes'].append(original_codes)\n",
    "\n",
    "    # Predicted codes are inferred from the model and need to be processed to match the format of original codes\n",
    "    predicted_codes = inference.replace(\",\", \" \").split()\n",
    "\n",
    "    # Check each predicted code against the list of legit codes before appending\n",
    "    # Removing periods from predicted codes only for lookup\n",
    "    legit_predicted_codes = [code for code in predicted_codes if code.replace(\".\", \"\") in legit_codes]\n",
    "    results2['Predicted Codes'].append(legit_predicted_codes)\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df2 = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(results_df2.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df2.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (['G3183', 'F0280', 'R441', 'R296', 'E785', 'Z8546'], ['I25.10', 'N18.9', 'E78.5', 'I10', 'K57.90', 'K21.9', 'E66.01', 'M19.90', 'C61', 'H25.9', 'Z48.813', 'Z95.1', 'Z87.891']),\n",
    "    (['C675', 'I10', 'D259', 'Z87891', 'E785', 'E890'], ['I49.8', 'I48.91', 'I25.10', 'I25.5', 'I47.2', 'G47.30']),\n",
    "    (['J441', 'N179', 'Z9981', 'I4891', 'D649', 'I10', 'E785', 'G5622', 'I2510', 'M1990', 'Z96649', 'Z87891', 'J45909', 'F419', 'G4700', 'R040', 'I739'], ['D62', 'D50.8', 'I73.9']),\n",
    "    (['K31811', 'B1910', 'S0990XA', 'G629', 'D62', 'F1120', 'I452', 'I6523', 'G40909', 'I951', 'F319', 'Q2733', 'I10', 'W01198A', 'Y92008', 'I701', 'M5416', 'E039', 'E785', 'J449', 'K219', 'Z86718', 'Z87891', 'K2270', 'R110', 'T402X5A', 'Y929', 'I739', 'I69398', 'R531', 'R42', 'N3090', 'R079', 'I459', 'K5900'], ['O80', 'Z37.0', 'O82', 'Z3A.37', 'N47.0']),\n",
    "    (['T8453XA', 'D62', 'N179', 'D709', 'B9562', 'D696', 'I10', 'E785', 'I2510', 'E860', 'H409', 'B9689', 'N400', 'Z951', 'Z8673', 'Z96652', 'Z954', 'Y792', 'Y929'], ['O09.212', 'Z33.1', 'J45.909', 'Z87.01', 'A54.9', 'O60.00'])\n",
    "]\n",
    "\n",
    "percent_diffs = []\n",
    "for left, right in data:\n",
    "    left_count = len(left)\n",
    "    right_count = len(right)\n",
    "    percent_diff = ((right_count - left_count) / left_count) * 100\n",
    "    percent_diffs.append(percent_diff)\n",
    "\n",
    "avg_percent_diff = sum(percent_diffs) / len(percent_diffs)\n",
    "\n",
    "# Print the average percentage difference\n",
    "print(f\"Average Percentage Difference: {avg_percent_diff:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT3.5 to structure/clean data -> GPT-4 inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_prompt = 'You are a medical coder. Return only clinically important text from this documentation that may influence ICD-10 coding. Pay special attention to areas describing present illness, chart review, imaging, discharge labs, active issues, medications, chief complaint, major surgery/procedure, etc.:'\n",
    "prompt ='You are an experienced medical coder. You must identify all correct ICD-10 codes for the following clinical note. Pay attention to areas describing present illness, chart review, imaging, discharge labs, active issues, medications, chief complaint, major surgery/procedure, etc. Return your answer in the following format, but note that the actual correct codes may vary greatly from these: I10, E78.5, Z87.891\\n'\n",
    "\n",
    "print(prompt)\n",
    "print(structure_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = ''\n",
    "\n",
    "def call_turbo(prompt):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\", \n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=600,           # Maximum number of tokens in the generated text. If the generated response exceeds this limit, it will be cut off.\n",
    "        temperature=1,          # Controls randomness. Higher values (closer to 1) make output more random, lower values (closer to 0) make it more deterministic.\n",
    "        top_p=1,                  # Sets the nucleus sampling value, controls diversity via probability threshold, can be used as an alternative to temperature.\n",
    "        frequency_penalty=0.0,    # Penalizes new tokens based on their frequency in the model's training data. Ranges from -2.0 to 2.0.\n",
    "        presence_penalty=0.0,     # Penalizes new tokens based on whether they appear in the context. Ranges from -2.0 to 2.0.\n",
    "        n=1,                      # The number of completions to generate. More completions means more diversity, but at a higher computational cost.\n",
    "        stream=False,             # If true, generate the response as a stream to reduce latency.\n",
    "        stop=None,                # A sequence (or list of sequences) where the API will stop generating further tokens.\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "# Empty dictionary to store the mapping\n",
    "results3 = {'Original Codes': [], 'Predicted Codes': []}\n",
    "\n",
    "note = sampled_df[['text']].iloc[i][0]\n",
    "processed_note = call_turbo(structure_prompt + note)\n",
    "\n",
    "for i in range(5):\n",
    "    note = sampled_df[['text']].iloc[i][0]\n",
    "    processed_note = call_turbo(structure_prompt + note)\n",
    "    inference = call_model(prompt + processed_note)\n",
    "    \n",
    "    # Append original codes and descriptions to the results\n",
    "    original_codes, _ = get_code_descriptions(i)\n",
    "    results3['Original Codes'].append(original_codes)\n",
    "\n",
    "    # Predicted codes are inferred from the model and need to be processed to match the format of original codes\n",
    "    predicted_codes = inference.replace(\",\", \" \").split()\n",
    "\n",
    "    # Check each predicted code against the list of legit codes before appending\n",
    "    # Removing periods from predicted codes only for lookup\n",
    "    legit_predicted_codes = [code for code in predicted_codes if code.replace(\".\", \"\") in legit_codes]\n",
    "    results3['Predicted Codes'].append(legit_predicted_codes)\n",
    "\n",
    "#Convert results to a DataFrame\n",
    "results_df3 = pd.DataFrame(results3)\n",
    "evaluate(results_df3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results3[\"Percentage Difference\"] = ((results_df2[\"Predicted Codes\"].apply(len) - results_df3[\"Original Codes\"].apply(len)) / results_df3[\"Original Codes\"].apply(len)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df3[\"Original Codes Count\"] = results_df3[\"Original Codes\"].apply(len)\n",
    "results_df3[\"Predicted Codes Count\"] = results_df3[\"Predicted Codes\"].apply(len)\n",
    "\n",
    "# Calculate the percentage difference\n",
    "results_df3[\"Percentage Difference\"] = ((results_df3[\"Predicted Codes Count\"] - results_df3[\"Original Codes Count\"]) / results_df3[\"Original Codes Count\"]) * 100\n",
    "\n",
    "# Calculate the average percentage difference\n",
    "avg_percent_diff = results_df3[\"Percentage Difference\"].mean()\n",
    "\n",
    "# Print the average percentage difference\n",
    "print(f\"Average Percentage Difference: {avg_percent_diff:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSP, KNN + GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dsp\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "\n",
    "locale.getpreferredencoding = getpreferredencoding\n",
    "pip install faiss-cpu\n",
    "pip install sentence-transformers\n",
    "\n",
    "dsp.settings.configure(vectorizer=dsp.SentenceTransformersVectorizer())\n",
    "knn_func = dsp.knn(squad_train)\n",
    "lm = dsp.GPT4(model='gpt-4', api_key=openai_key)\n",
    "\n",
    "@dsp.transformation\n",
    "def inf(example, k=3):\n",
    "    knn_res_train_vec = knn_func(example, k) # get k demos\n",
    "    example.demos = knn_res_train_vec\n",
    "    example.context = dsp.retrieve(example.question, k=2)\n",
    "    example, completions = dsp.generate(qa_template_with_passages, temperature=0.6)(example, stage='qa')\n",
    "    return completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
